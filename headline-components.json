#!/usr/bin/env python3
"""
LuckNooz Headline Generator with spaCy
Enhanced verb detection and subject-verb agreement
"""

import feedparser
import json
from datetime import datetime
import spacy

# Load spaCy English model
print("Loading spaCy model...")
nlp = spacy.load('en_core_web_sm')

# RSS feeds
FEEDS = [
    'https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml',
    'https://feeds.bbci.co.uk/news/world/rss.xml',
    'https://feeds.a.dj.com/rss/RSSWorldNews.xml',
    'https://www.theguardian.com/world/rss',
    'https://www.nature.com/nature.rss',
    'https://variety.com/feed/'
]

# Question words to skip
QUESTION_WORDS = {'who', 'what', 'when', 'where', 'why', 'how', 'which', 'whose'}

def extract_subject_and_verb(headline):
    """Extract subject and its main verb using spaCy"""
    doc = nlp(headline)
    
    # Find the root verb
    root_verb = None
    for token in doc:
        if token.dep_ == 'ROOT' and token.pos_ == 'VERB':
            root_verb = token
            break
    
    if not root_verb:
        return None, None
    
    # Find the subject of the root verb
    subject_tokens = []
    for token in doc:
        if token.head == root_verb and token.dep_ in ['nsubj', 'nsubjpass']:
            # Get the full noun phrase
            subject_tokens = list(token.subtree)
            break
    
    if not subject_tokens:
        return None, None
    
    # Build subject text
    subject_text = ' '.join([t.text for t in sorted(subject_tokens, key=lambda x: x.i)])
    
    # Check if subject starts with question word - skip if so
    if subject_tokens[0].text.lower() in QUESTION_WORDS:
        return None, None
    
    return subject_text, root_verb

def extract_predicate(headline, subject_text):
    """Extract predicate (everything after subject)"""
    if not subject_text:
        return None
    
    # Find where subject ends in headline
    subject_end = headline.find(subject_text) + len(subject_text)
    predicate = headline[subject_end:].strip()
    
    # Remove leading punctuation/conjunctions
    predicate = predicate.lstrip(':;,-')
    predicate = predicate.strip()
    
    if len(predicate) < 10:  # Too short
        return None
    
    return predicate

def find_predicate_verb(predicate_text):
    """Find the main verb in the predicate using spaCy"""
    doc = nlp(predicate_text)
    
    # Skip question words at start
    start_idx = 0
    if doc and doc[0].text.lower() in QUESTION_WORDS:
        start_idx = 1
    
    # Find first real verb (not auxiliary, not gerund at start)
    for token in doc[start_idx:]:
        # Skip auxiliary verbs (is, are, was, were, has, have, had)
        if token.pos_ == 'AUX':
            continue
        
        # Find main verb
        if token.pos_ == 'VERB':
            # Skip gerunds at the very beginning (they might be part of title)
            if token.i == 0 and token.tag_ == 'VBG':
                continue
            return token.text, token.lemma_, token.tag_
    
    return None, None, None

def conjugate_verb(lemma, tag):
    """Conjugate a verb based on the target tag"""
    # For past tense
    if tag == 'VBD':
        # Special cases
        if lemma in ['be', 'is', 'are']: return 'was'
        if lemma in ['have', 'has']: return 'had'
        # Regular past tense
        if lemma.endswith('e'):
            return lemma + 'd'
        elif lemma.endswith('y') and len(lemma) > 1 and lemma[-2] not in 'aeiou':
            return lemma[:-1] + 'ied'
        else:
            return lemma + 'ed'
    
    # For present 3rd person singular
    elif tag == 'VBZ':
        # Special cases
        if lemma == 'be': return 'is'
        if lemma == 'have': return 'has'
        # Regular 3rd person
        if lemma.endswith(('s', 'sh', 'ch', 'x', 'z', 'o')):
            return lemma + 'es'
        elif lemma.endswith('y') and len(lemma) > 1 and lemma[-2] not in 'aeiou':
            return lemma[:-1] + 'ies'
        else:
            return lemma + 's'
    
    # For present non-3rd person (base form)
    elif tag in ['VBP', 'VB']:
        return lemma
    
    return lemma

def match_verb_tense(subject_verb, predicate_verb_info):
    """Match predicate verb tense to subject verb tense"""
    if not predicate_verb_info[0]:
        return None
    
    pred_verb, pred_lemma, pred_tag = predicate_verb_info
    subject_tense = subject_verb.tag_
    
    # If predicate verb already matches subject tense, return as-is
    if subject_tense == pred_tag:
        return pred_verb
    
    # Match predicate to subject tense
    if subject_tense == 'VBD':  # Past tense subject
        return conjugate_verb(pred_lemma, 'VBD')
    
    elif subject_tense == 'VBZ':  # Present 3rd singular subject (honors, is, says)
        return conjugate_verb(pred_lemma, 'VBZ')
    
    elif subject_tense == 'VBP':  # Present non-3rd person
        return conjugate_verb(pred_lemma, 'VBP')
    
    elif subject_tense == 'VB':  # Base form
        return conjugate_verb(pred_lemma, 'VB')
    
    # Default: return original
    return pred_verb

def process_feed(feed_url):
    """Process a single RSS feed"""
    print(f"Fetching {feed_url}...")
    try:
        feed = feedparser.parse(feed_url)
        subjects = []
        predicates = []
        
        for entry in feed.entries[:20]:  # Process first 20
            title = entry.get('title', '')
            if not title or len(title) < 20:
                continue
            
            # Extract subject and its verb
            subject_text, subject_verb = extract_subject_and_verb(title)
            
            if subject_text and subject_verb:
                # Store subject
                subjects.append({
                    'text': subject_text,
                    'verb': subject_verb.text,
                    'source': feed_url.replace('https://', '').replace('http://', '').split('/')[0],
                    'category': 'core_news',
                    'original': title
                })
                
                # Extract and process predicate
                predicate_text = extract_predicate(title, subject_text)
                
                if predicate_text:
                    pred_verb_info = find_predicate_verb(predicate_text)
                    matched_verb = match_verb_tense(subject_verb, pred_verb_info) if pred_verb_info[0] else None
                    
                    predicates.append({
                        'text': predicate_text,
                        'verb': matched_verb or pred_verb_info[0] if pred_verb_info[0] else 'unknown',
                        'source': feed_url.replace('https://', '').replace('http://', '').split('/')[0],
                        'category': 'core_news',
                        'original': title
                    })
        
        return subjects, predicates
    
    except Exception as e:
        print(f"Error processing {feed_url}: {e}")
        return [], []

def main():
    print("=" * 60)
    print("LuckNooz Headline Generator with spaCy")
    print("=" * 60)
    
    all_subjects = []
    all_predicates = []
    
    # Process all feeds
    for feed_url in FEEDS:
        subjects, predicates = process_feed(feed_url)
        all_subjects.extend(subjects)
        all_predicates.extend(predicates)
    
    # Create output
    output = {
        'subjects': all_subjects,
        'predicates': all_predicates,
        'generated': datetime.now().isoformat(),
        'total_subjects': len(all_subjects),
        'total_predicates': len(all_predicates)
    }
    
    # Save to file
    output_file = 'headline-components.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 60)
    print(f"✅ Generated {len(all_subjects)} subjects")
    print(f"✅ Generated {len(all_predicates)} predicates")
    print(f"✅ Saved to {output_file}")
    print("=" * 60)
    
    # Show some examples
    print("\nSample subjects:")
    for subj in all_subjects[:5]:
        print(f"  - {subj['text']} ({subj['verb']})")
    
    print("\nSample predicates:")
    for pred in all_predicates[:5]:
        print(f"  - {pred['text']} ({pred['verb']})")

if __name__ == '__main__':
    main()
